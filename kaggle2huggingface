{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4494251,"sourceType":"datasetVersion","datasetId":1623201}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport glob\nimport random\nimport cv2\nimport numpy as np\nimport json\nimport pandas as pd\nimport shutil\nimport os\nimport zipfile\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import upload_file, HfApi\nHF_APIKEY = UserSecretsClient().get_secret(\"HF-APIKEY\")\napi = HfApi(token=HF_APIKEY)\n\nSRC_DIR = '/kaggle/input/pollen20ldet/'\nIMAGE_DIR = '/kaggle/input/pollen20ldet/images'\nDEST_DIR = 'data/dataset'\nPHASES = [\"train\", \"valid\", \"test\"]\nTRAIN_DIRECTORY = DEST_DIR + '/train'\nVAL_DIRECTORY = DEST_DIR + '/valid'\nTEST_DIRECTORY = DEST_DIR + '/test'","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:52:18.684622Z","iopub.execute_input":"2024-04-11T16:52:18.685360Z","iopub.status.idle":"2024-04-11T16:52:18.828877Z","shell.execute_reply.started":"2024-04-11T16:52:18.685313Z","shell.execute_reply":"2024-04-11T16:52:18.827811Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess and Split\n\n- Split into train/valid/test sets\n- Convert from Pascal VOC (csv) to COCO JSON format \n- Zip each phase\n  - train/*.jpg (images)\n  - train/train.json (annotations)\n  - valid/*\n  - test/*","metadata":{}},{"cell_type":"code","source":"shutil.copyfile(SRC_DIR, DEST_DIR+'/full')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_voc_to_yolo(voc_bbox, img_width, img_height):\n    x_min, y_min, x_max, y_max = voc_bbox\n    x_center = (x_min + x_max) / 2 / img_width\n    y_center = (y_min + y_max) / 2 / img_height\n    width = (x_max - x_min) / img_width\n    height = (y_max - y_min) / img_height\n    return x_center, y_center, width, height\n\nclass DatasetPreprocessor:\n    def __init__(self, src_dir, image_dir, dest_dir, phases=[\"train\",\"valid\",\"test\"]):\n        self.src_dir = src_dir\n        self.image_dir = image_dir\n        self.dest_dir = dest_dir\n        self.phases = phases\n        self.annotation_file_name = \".json\"\n        \n    def zip_each_phase(self):\n        for phase in self.phases:\n            phase_dir = os.path.join(self.dest_dir, phase)\n            zip_file = os.path.join(self.dest_dir, f\"{phase}.zip\")\n            with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for root, _, files in os.walk(phase_dir):\n                    for file in files:\n                        zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), phase_dir))\n\n    def split_dataset(self, dataset_df):\n        # Define the percentages for train, valid, and test splits\n        train_percentage = 0.75\n        valid_percentage = 0.15\n        test_percentage = 0.10\n\n        full_train_images = []\n        full_valid_images = []\n        full_test_images = []\n\n        for class_name, sub_df in dataset_df.groupby(['class_name']):\n            num_images = sub_df.shape[0]\n            # Ensure at least one image in each phase\n            num_train = max(1, int(train_percentage * num_images))\n            num_valid = max(1, int(valid_percentage * num_images))\n            num_test = max(1, int(test_percentage * num_images))\n\n            # Take random sample for test and valid sets. Train will be leftover\n\n            test_images = list(sub_df.sample(n=num_test, random_state=1).image_name.values)\n            valid_images = list(sub_df[~sub_df.image_name.isin(test_images)].sample(n=num_valid, random_state=1).image_name.values)\n            train_images = list(sub_df[~sub_df.image_name.isin(valid_images + test_images)].image_name.values)\n            # Create directories for train, valid, and test splits\n            full_train_images.extend(train_images)\n            full_valid_images.extend(valid_images)\n            full_test_images.extend(test_images)\n            \n        if len(self.phases) == 2:\n            full_train_images = full_train_images + full_test_images\n            images_phases = zip(self.phases, [full_train_images, full_valid_images])\n        else:\n            images_phases = zip(self.phases, [full_train_images, full_valid_images, full_test_images])\n\n        for phase, image_names in images_phases:\n            os.makedirs(os.path.join(self.dest_dir, phase), exist_ok=True)\n            for image_name in image_names:\n                src_path = os.path.join(self.image_dir, image_name)\n                dest_path = os.path.join(self.dest_dir, phase, image_name)\n                shutil.copyfile(src_path, dest_path)\n\n    def annotate_dataset(self, categories, bboxes_df, inverse_class_dict):\n        for phase in self.phases:\n            root_path = os.path.join(self.dest_dir, phase)\n            json_file = os.path.join(self.dest_dir, f\"{phase}/_annotations.json\")\n            res_file = {\n                \"categories\": categories,\n                \"images\": [],\n                \"annotations\": []\n            }\n            annot_count = 0\n            image_id = 0\n            processed = 0\n\n            # Obtain image_names\n            file_list = glob.glob(os.path.join(root_path, \"*.jpg\"))\n\n            for file in file_list:\n                image_path = file\n                file_name = file.replace(root_path, \"\").replace(\"\\\\\", '').replace('/', '')\n                img = cv2.imread(image_path)\n                img_h, img_w, channels = img.shape\n                img_elem = {\n                    \"file_name\": file_name,\n                    \"height\": img_h,\n                    \"width\": img_w,\n                    \"id\": image_id\n                }\n                res_file[\"images\"].append(img_elem)\n                annotations = bboxes_df[bboxes_df.image_name == file_name]\n                for idx, row in annotations.iterrows():\n                    key = row['image_name']\n                    voc_bbox = row['x1'], row['y1'], row['x2'], row['y2']\n                    x1, y1, x2, y2 = convert_voc_to_yolo(voc_bbox, img_w, img_h)\n                    coords = inverse_class_dict.get(row['class_name']), x1, y1, x2, y2\n\n                    # YOLO to COCO JSON\n                    x_center = (float(coords[1]) * (img_w))\n                    y_center = (float(coords[2]) * (img_h))\n                    width = (float(coords[3]) * img_w)\n                    height = (float(coords[4]) * img_h)\n                    category_id = int(coords[0])  # label\n\n                    mid_x = int(x_center - width / 2)\n                    mid_y = int(y_center - height / 2)\n                    width = int(width)\n                    height = int(height)\n\n                    area = width * height\n                    poly = [[mid_x, mid_y],\n                            [width, height],\n                            [width, height],\n                            [mid_x, mid_y]]\n\n                    annot_elem = {\n                        \"id\": annot_count,\n                        \"bbox\": [\n                            float(mid_x),\n                            float(mid_y),\n                            float(width),\n                            float(height)\n                        ],\n                        \"segmentation\": list([poly]),\n                        \"image_id\": image_id,\n                        \"ignore\": 0,\n                        \"category_id\": category_id,\n                        \"iscrowd\": 0,\n                        \"area\": float(area)\n                    }\n                    res_file[\"annotations\"].append(annot_elem)\n                    annot_count += 1\n                image_id += 1\n\n                processed += 1\n            with open(json_file, \"w\") as f:\n                json_str = json.dumps(res_file)\n                f.write(json_str)\n\n            print(\"Processed {} {} images...\".format(processed, phase))\n        print(\"Done.\")\n\n    def process_dataset(self, bboxes_file, class_map_file, dataloader_batchsize=2):\n        print('Processing Dataset...')\n        bboxes_df = pd.read_csv(bboxes_file, header=None, names=['image_name', 'x1', 'y1', 'x2', 'y2', 'class_name'])\n        \n        labels_df = pd.read_csv(class_map_file, header=None, names=['name', 'id'])\n        class_dict = dict(zip(list(map(int, labels_df.id.values)), list(labels_df.name.values)))\n        inverse_class_dict = dict(zip(list(labels_df.name.values), list(map(int,labels_df.id.values))))\n        categories = [{\"supercategory\": \"none\", \"name\": val, \"id\": int(key)} for key, val in class_dict.items()]\n\n        dataset_df = bboxes_df[['image_name', 'class_name']].drop_duplicates(['image_name'])\n        dataset_df.class_name.value_counts()\n        print(f'  -- Images: {dataset_df.shape[0]}')\n        print(f'  -- Annotations: {bboxes_df.shape[0]}')\n        print(f'  -- Classes: {labels_df.shape[0]}')\n        print('')\n        print('Splitting Dataset...')\n        self.split_dataset(dataset_df)\n        print('Processing Dataset...')\n        self.annotate_dataset(categories, bboxes_df, inverse_class_dict)\n        print('Zipping each phase...')\n        self.zip_each_phase()\n        \n        ### ZIP EACH self.phases (train, valid, test)\n        return class_dict, inverse_class_dict\n        \n        \ndataset_preprocessor = DatasetPreprocessor(\n    src_dir=SRC_DIR,\n    image_dir=IMAGE_DIR,\n    dest_dir=DEST_DIR,\n    phases=PHASES\n)\nid2label, label2id = dataset_preprocessor.process_dataset(\n    bboxes_file=f\"{SRC_DIR}bboxes.csv\",\n    class_map_file=f\"{SRC_DIR}class_map.csv\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Dataset Loader","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/pollen_detection_loader.py\n\nimport collections\nimport json\nimport os\n\nimport datasets\n\n\n_HOMEPAGE = \"https://www.kaggle.com/datasets/nataliakhanzhina/pollen20ldet\"\n_LICENSE = \"CC BY 4.0\"\n_CITATION = \"\"\"\\\n@misc{ pollen20ldet,\n    title = { Combating data incompetence in pollen images detection and classification for pollinosis prevention },\n    type = { Open Source Dataset },\n    author = { Khanzhina, Natalia and Filchenkov, Andrey and Minaeva, Natalia and Novoselova, Larisa and Petukhov, Maxim and Kharisova, Irina and Pinaeva, Julia and Zamorin, Georgiy and Putin, Evgeny and Zamyatina, Elena and others},\n    howpublished = { \\\\url{ https://www.kaggle.com/datasets/nataliakhanzhina/pollen20ldet } },\n    url = { https://www.kaggle.com/datasets/nataliakhanzhina/pollen20ldet },\n    journal = { Computers in biology and medicine },\n    volume={140},\n    pages={105064},\n    publisher = { Elsevier },\n    year = { 2022 },\n}\n\"\"\"\n\n### I want to look at multiple ways to load categories. Pollen has multiple classification levels and is usually classified via latin version of the term\n\n_CATEGORIES = [\n    'buckwheat',\n    'clover',\n    'angelica',\n    'angelica_garden',\n    'willow',\n    'hill_mustard',\n    'linden',\n    'meadow_pink',\n    'alder',\n    'birch',\n    'fireweed',\n    'nettle',\n    'pigweed',\n    'plantain',\n    'sorrel',\n    'grass',\n    'pine',\n    'maple',\n    'hazel',\n    'mugwort'\n]\n\n_ANNOTATION_FILENAME = \"_annotations.json\"\n\n\nclass POLLENDETECTIONConfig(datasets.BuilderConfig):\n    \"\"\"Builder Config for pollen-detection\"\"\"\n\n    def __init__(self, data_urls, **kwargs):\n        \"\"\"\n        BuilderConfig for pollen-detection.\n        Args:\n          data_urls: `dict`, name to url to download the zip file from.\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        super(POLLENDETECTIONConfig, self).__init__(version=datasets.Version(\"1.0.0\"), **kwargs)\n        self.data_urls = data_urls\n\n\nclass POLLENDETECTION(datasets.GeneratorBasedBuilder):\n    \"\"\"pollen-detection object detection dataset\"\"\"\n\n    VERSION = datasets.Version(\"1.0.0\")\n    BUILDER_CONFIGS = [\n        POLLENDETECTIONConfig(\n            name=\"full\",\n            description=\"Full version of pollen-detection dataset.\",\n            data_urls={\n                \"train\": \"https://huggingface.co/datasets/Charliesgt/Pollen20LDet/resolve/main/data/train.zip\",\n                \"valid\": \"https://huggingface.co/datasets/Charliesgt/Pollen20LDet/resolve/main/data/valid.zip\",\n                \"test\": \"https://huggingface.co/datasets/Charliesgt/Pollen20LDet/resolve/main/data/test.zip\",\n            },\n        ),\n    ]\n\n    def _info(self):\n        features = datasets.Features(\n            {\n                \"image_id\": datasets.Value(\"int64\"),\n                \"image\": datasets.Image(),\n                \"width\": datasets.Value(\"int32\"),\n                \"height\": datasets.Value(\"int32\"),\n                \"objects\": datasets.Sequence(\n                    {\n                        \"id\": datasets.Value(\"int64\"),\n                        \"area\": datasets.Value(\"int64\"),\n                        \"bbox\": datasets.Sequence(datasets.Value(\"float32\"), length=4),\n                        \"category\": datasets.ClassLabel(names=_CATEGORIES),\n                    }\n                ),\n            }\n        )\n        return datasets.DatasetInfo(\n            features=features,\n            homepage=_HOMEPAGE,\n            citation=_CITATION,\n            license=_LICENSE,\n        )\n\n    def _split_generators(self, dl_manager):\n        data_files = dl_manager.download_and_extract(self.config.data_urls)\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\n                    \"folder_dir\": data_files[\"train\"],\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\n                    \"folder_dir\": data_files[\"valid\"],\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"folder_dir\": data_files[\"test\"],\n                },\n            ),\n]\n\n    def _generate_examples(self, folder_dir):\n        def process_annot(annot, category_id_to_category):\n            return {\n                \"id\": annot[\"id\"],\n                \"area\": annot[\"area\"],\n                \"bbox\": annot[\"bbox\"],\n                \"category\": category_id_to_category[annot[\"category_id\"]],\n            }\n\n        image_id_to_image = {}\n        idx = 0\n\n        annotation_filepath = os.path.join(folder_dir, _ANNOTATION_FILENAME)\n        with open(annotation_filepath, \"r\") as f:\n            annotations = json.load(f)\n        category_id_to_category = {category[\"id\"]: category[\"name\"] for category in annotations[\"categories\"]}\n        image_id_to_annotations = collections.defaultdict(list)\n        for annot in annotations[\"annotations\"]:\n            image_id_to_annotations[annot[\"image_id\"]].append(annot)\n        filename_to_image = {image[\"file_name\"]: image for image in annotations[\"images\"]}\n\n        for filename in os.listdir(folder_dir):\n            filepath = os.path.join(folder_dir, filename)\n            if filename in filename_to_image:\n                image = filename_to_image[filename]\n                objects = [\n                    process_annot(annot, category_id_to_category) for annot in image_id_to_annotations[image[\"id\"]]\n                ]\n                with open(filepath, \"rb\") as f:\n                    image_bytes = f.read()\n                yield idx, {\n                    \"image_id\": image[\"id\"],\n                    \"image\": {\"path\": filepath, \"bytes\": image_bytes},\n                    \"width\": image[\"width\"],\n                    \"height\": image[\"height\"],\n                    \"objects\": objects,\n                }\n                idx += 1\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your dataset name\n\ndataset_name = \"Pollen20LDet\"\n\nrepo_id = f\"Charliesgt/{dataset_name}\"\nrepo_type='dataset'\n\n# Define paths to your files\ntrain_zip_path = f\"{DEST_DIR}/train.zip\"\nvalid_zip_path = f\"{DEST_DIR}/valid.zip\"\ntest_zip_path = f\"{DEST_DIR}/test.zip\"\nloader_script_path = \"/kaggle/working/pollen_detection_loader.py\"\n\n# Create directory structure\ndata_dir = os.path.join(dataset_name, \"data\")\nos.makedirs(data_dir, exist_ok=True)\n\n# Move zip files to data folder\nshutil.copy(train_zip_path, os.path.join(data_dir, \"train.zip\"))\nshutil.copy(valid_zip_path, os.path.join(data_dir, \"valid.zip\"))\nshutil.copy(test_zip_path, os.path.join(data_dir, \"test.zip\"))\n\n# Upload dataset files to Hugging Face Dataset Hub\napi.upload_folder(folder_path=data_dir,path_in_repo = 'data', repo_id=repo_id, repo_type=repo_type)\n\n# Upload dataset loader script\napi.upload_file(path_or_fileobj=loader_script_path,path_in_repo='Pollen20LDet.py',repo_id=repo_id, repo_type=repo_type, commit_message=\"Upload dataset loader script\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict\ndataset = load_dataset(\"Charliesgt/Pollen20LDet\", trust_remote_code=True)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport json\nimport glob\ndef load_dataset_local(repo_id ,splits, dest_dir):\n    if os.path.exists(f'/kaggle/working/{dest_dir}'):\n        shutil.rmtree(f'/kaggle/working/{dest_dir}')\n    \n    split_path = f\"https://huggingface.co/datasets/{repo_id}/resolve/main/data/\"\n    os.makedirs(dest_dir, exist_ok=True)\n    for split in splits:\n        # Download the zip file\n        response = requests.get(split_path + f\"{split}.zip\")\n        os.makedirs(os.path.join(dest_dir, split), exist_ok=True)\n        \n        # Save the zip file\n        save_dir = os.path.join(dest_dir, split)\n        zip_file_path = os.path.join(save_dir, f\"{split}.zip\")\n        with open(zip_file_path, \"wb\") as f:\n            f.write(response.content)\n        # Unzip the downloaded file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(save_dir)\n\n        # Remove the zip file if you no longer need it\n        os.remove(zip_file_path)\n\nload_dataset_local('Charliesgt/Pollen20LDet',PHASES, DEST_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:57:33.291694Z","iopub.execute_input":"2024-04-11T16:57:33.292085Z","iopub.status.idle":"2024-04-11T16:57:34.983125Z","shell.execute_reply.started":"2024-04-11T16:57:33.292056Z","shell.execute_reply":"2024-04-11T16:57:34.982015Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PollenDetection(torchvision.datasets.CocoDetection):\n    def __init__(\n        self, \n        image_directory_path: str, \n        image_processor, \n        train: bool = True\n    ):\n        annotation_file_path = image_directory_path + '.json'\n        super(PollenDetection, self).__init__(image_directory_path, annotation_file_path)\n        self.image_processor = image_processor\n\n    def __getitem__(self, idx):\n        images, annotations = super(PollenDetection, self).__getitem__(idx)        \n        image_id = self.ids[idx]\n        annotations = {'image_id': image_id, 'annotations': annotations}\n        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze()\n        target = encoding[\"labels\"][0]\n        return pixel_values, target\n    \n    \n    \n        def collate_fn(batch):\n            # DETR authors employ various image sizes during training, making it not possible\n            # to directly batch together images. Hence they pad the images to the biggest\n            # resolution in a given batch, and create a corresponding binary pixel_mask\n            # which indicates which pixels are real/which are padding\n            pixel_values = [item[0] for item in batch]\n            encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n            labels = [item[1] for item in batch]\n            return {\n                'pixel_values': encoding['pixel_values'],\n                'pixel_mask': encoding['pixel_mask'],\n                'labels': labels\n            }\n\n        train_dataset = PollenDetection(\n            image_directory_path=TRAIN_DIRECTORY,\n            image_processor=image_processor,\n            train=True\n        )\n        val_dataset = PollenDetection(\n            image_directory_path=VAL_DIRECTORY,\n            image_processor=image_processor,\n            train=False\n        )\n        train_dataloader = DataLoader(dataset=train_dataset, collate_fn=collate_fn, batch_size=dataloader_batchsize, shuffle=True, num_workers=4)\n        val_dataloader = DataLoader(dataset=val_dataset, collate_fn=collate_fn, batch_size=dataloader_batchsize, num_workers=2)\n        return train_dataloader, val_dataloader, class_dict, inverse_class_dict","metadata":{},"execution_count":null,"outputs":[]}]}